{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "editorial-parts",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%run Preprocessing.ipynb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "theoretical-official",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from sklearn import tree\n",
    "import sklearn.feature_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import statistics\n",
    "import graphviz\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accessory-pulse",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_train = x_train\n",
    "y_train = y_train\n",
    "x_test = x_test #final valication with the kfold\n",
    "y_test = y_test #final valication with the kfold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-stereo",
   "metadata": {},
   "source": [
    "**Feature Selection**\n",
    "- Build the model using the pre-processed data\n",
    "\n",
    "In order to avoid overfitting and slow computing (due to the increase in features from dummying as well as increasing dimensionality), selecting the most important features is important. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "conscious-orbit",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['PctKids2Par',\n 'racePctWhite',\n 'PctKidsBornNeverMar',\n 'PctFam2Par',\n 'PctYoungKids2Par',\n 'PctTeen2Par',\n 'racepctblack',\n 'pctWInvInc',\n 'pctWPubAsst',\n 'PctPersOwnOccup',\n 'PctPopUnderPov',\n 'FemalePctDiv',\n 'PctNotHSGrad',\n 'PctHousNoPhone',\n 'TotalPctDiv',\n 'MalePctDivorce',\n 'PctPersDenseHous',\n 'PctHousOwnOcc',\n 'PctHousLess3BR',\n 'medFamInc']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select k best is a univariate method for feature selection:\n",
    "# looks at the outcome and the relationship with each feature and selects k number of best features \n",
    "select = sklearn.feature_selection.SelectKBest(k=20)\n",
    "selected_features = select.fit(x_train, y_train)\n",
    "indices_selected = selected_features.get_support(indices=True)\n",
    "colnames_selected = [x.columns[I] for I in indices_selected]\n",
    "\n",
    "x_train_selected = x_train[colnames_selected]\n",
    "x_test_selected = x_test[colnames_selected]\n",
    "\n",
    "colnames_selected # 20 features selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-woman",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "scrolled": false
   },
   "source": [
    "**Build Decision tree #1**\n",
    "\n",
    "Firstly, we create a basic DecisionTreeClassifier, and then will slowly tune the parameters, firstly, we need to look at the **max depth**. The cell below will calcaulate the accuracy score for a max depth up to *n*. This is then plotted in a bar graph.\n",
    "\n",
    "For each parameter, we need to create a k-means loop, that then plots the overall figure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abroad-coordination",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'j' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-6-bc1210ba5cae>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mkf\u001B[0m \u001B[1;33m=\u001B[0m\u001B[0mKFold\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn_splits\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshuffle\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrandom_state\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m42\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mscore\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcross_val_score\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtree\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDecisionTreeClassifier\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcriterion\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'entropy'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_depth\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcv\u001B[0m\u001B[1;33m=\u001B[0m \u001B[0mkf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscoring\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"accuracy\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mscore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mkf\u001B[0m \u001B[1;33m=\u001B[0m\u001B[0mKFold\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn_splits\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshuffle\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrandom_state\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m42\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'j' is not defined"
     ]
    }
   ],
   "source": [
    "kf =KFold(n_splits=j, shuffle=True, random_state=42)\n",
    "score = cross_val_score(tree.DecisionTreeClassifier(criterion='entropy', max_depth=i), x_train, y_train, cv= kf, scoring=\"accuracy\")\n",
    "print(score.mean())\n",
    "\n",
    "kf =KFold(n_splits=j, shuffle=True, random_state=42)\n",
    "score = cross_val_score(tree.DecisionTreeClassifier(criterion='gini', max_depth=i), x_train, y_train, cv= kf, scoring=\"accuracy\")\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-reality",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_max_depth(n):\n",
    "    scores ={}\n",
    "    x = list()\n",
    "    y = list()\n",
    "    error = list()\n",
    "    \n",
    "    for i in range(3,n): #first create a for loop for each max depth\n",
    "        value_of_j = [] #this will take the average score for each value of k\n",
    "        \n",
    "        for j in range(2,10): #create a for loop for each kfold\n",
    "            kf =KFold(n_splits=j, shuffle=True, random_state=42)\n",
    "            score = cross_val_score(tree.DecisionTreeClassifier(criterion='gini', max_depth=i), x_train, y_train, cv= kf, scoring=\"accuracy\")\n",
    "            value_of_j.append(score.mean())\n",
    "            \n",
    "        res = {str(i): value_of_j}\n",
    "        scores.update(res) # for each max_depth, the average k_fold score\n",
    "  \n",
    "    return scores\n",
    "            \n",
    "d_max_depth = get_max_depth(10)\n",
    "\n",
    "x =[]\n",
    "y =[]\n",
    "error = []\n",
    "for key, value in d_max_depth.items():\n",
    "    x.append(key)\n",
    "    y.append(sum(value)/len(value))\n",
    "    error.append(statistics.stdev(value))\n",
    "    \n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "# plt.plot(x, error, 'or')\n",
    "plt.bar(x, y, color=(0.2, 0.4, 0.6, 0.6))\n",
    "plt.errorbar(x, y, yerr = error, barsabove = False, fmt='o', capsize=3, color='red')\n",
    "#plt.errorbar(x, y, yerr = error, ls='-.', fmt='o', capsize=3)\n",
    "plt.xlabel('Max_Depth', fontsize=15)\n",
    "plt.ylabel('Accuracy', fontsize=15)\n",
    "plt.title('Change in Accuracy as Max Depth increases')\n",
    "plt.ylim(0.8, 0.86)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-rendering",
   "metadata": {},
   "source": [
    "For this dataset, we have shown that the max_depth at 6 provides the highest accuracy. Therefore, the current model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-senegal",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores ={}\n",
    "x = list()\n",
    "y = list()\n",
    "error = list()\n",
    "\n",
    "min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\n",
    "\n",
    "for i in min_samples_splits: #first create a for loop for each max depth\n",
    "    value_of_j = [] #this will take the average score for each value of k\n",
    "\n",
    "    for j in range(2,10): #create a for loop for each kfold\n",
    "        kf =KFold(n_splits=j, shuffle=True, random_state=42)\n",
    "        score = cross_val_score(tree.DecisionTreeClassifier(criterion='gini', splitter = 'best', max_depth=3, min_samples_split = i), x_train, y_train, cv= kf, scoring=\"accuracy\")\n",
    "        value_of_j.append(score.mean())\n",
    "\n",
    "    res = {str(i): value_of_j}\n",
    "    scores.update(res) # for each max_depth, the average k_fold score\n",
    "\n",
    "get_min_sample_split = scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "x =[]\n",
    "y =[]\n",
    "error = []\n",
    "for key, value in get_min_sample_split.items():\n",
    "    x.append(key)\n",
    "    y.append(sum(value)/len(value))\n",
    "    error.append(statistics.stdev(value))\n",
    "    \n",
    "x = [round(float(i),2) for i in x]\n",
    "    \n",
    "# plt.plot(x, error, 'or')\n",
    "plt.plot(x, y, color=(0.2, 0.4, 0.6, 0.6), lw=3)\n",
    "plt.errorbar(x, y, yerr = error, barsabove = False, fmt='o', capsize=3, color='red')\n",
    "#plt.errorbar(x, y, yerr = error, ls='-.', fmt='o', capsize=3)\n",
    "plt.xlabel('Min_samples_split', fontsize=15)\n",
    "plt.ylabel('Accuracy', fontsize=15)\n",
    "plt.title('Change in Accuracy as min_samples_split increases')\n",
    "plt.ylim(0.8, 0.86)\n",
    "plt.xlim(0,1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(criterion='gini', splitter = 'best', max_depth=6, min_samples_split =0.1)\n",
    "dtree = dtree.fit(x_train, y_train)\n",
    "#tree.plot_tree(dtree, feature_names=data.columns) \n",
    "\n",
    "# model can then be used to predict !\n",
    "# crime_predict = dtree.predict([[0,1]])\n",
    "\n",
    "y_pred = dtree.predict(x_test)  # Predict the response for test dataset\n",
    "print(\"Accuracy:\",accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-handling",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the tree\n",
    "features = []\n",
    "df = pd.DataFrame(x_train)\n",
    "for i in df[1:]:\n",
    "    features.append(str(i))\n",
    "dot_data = tree.export_graphviz(dtree, out_file=None, feature_names=features, filled=True, rounded=True, special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-uniform",
   "metadata": {},
   "source": [
    "code adapted from https://medium.com/@mohtedibf/indepth-parameter-tuning-for-decision-tree-6753118a03c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores ={}\n",
    "x = list()\n",
    "y = list()\n",
    "error = list()\n",
    "\n",
    "features = []\n",
    "df = pd.DataFrame(x_train)\n",
    "for i in df[1:]:\n",
    "    features.append(str(i))\n",
    "\n",
    "max_features = len(features)\n",
    "\n",
    "for i in range(1, max_features): #first create a for loop for each max depth\n",
    "    value_of_j = [] #this will take the average score for each value of k\n",
    "\n",
    "    for j in range(2,10): #create a for loop for each kfold\n",
    "        kf =KFold(n_splits=j, shuffle=True, random_state=42)\n",
    "        score = cross_val_score(tree.DecisionTreeClassifier(criterion='gini', splitter = 'best', max_depth=3, min_samples_split = 0.1, max_features=i), x_train, y_train, cv= kf, scoring=\"accuracy\")\n",
    "        value_of_j.append(score.mean())\n",
    "\n",
    "    res = {str(i): value_of_j}\n",
    "    scores.update(res) # for each max_depth, the average k_fold score\n",
    "\n",
    "get_min_sample_split = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_min_sample_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-highway",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "x =[]\n",
    "y =[]\n",
    "error = []\n",
    "for key, value in get_min_sample_split.items():\n",
    "    x.append(key)\n",
    "    y.append(sum(value)/len(value))\n",
    "    error.append(statistics.stdev(value))\n",
    "    \n",
    "x = [round(float(i),2) for i in x]\n",
    "\n",
    "y_above = []\n",
    "y_below = []\n",
    "for i in range(len(error)):\n",
    "    y_above.append(y[i]+(error[i]*0.5))\n",
    "    y_below.append(y[i]-(error[i]*0.5))\n",
    "    \n",
    "# plt.plot(x, error, 'or')\n",
    "plt.plot(x, y, color=(0.2, 0.4, 0.6, 0.6), lw=3)\n",
    "#plt.errorbar(x, y, yerr = error, barsabove = False, fmt='o', capsize=3, color='red')\n",
    "\n",
    "#plt.fill_between(range(100), y-y_diff, y+y_diff, alpha=0.5)\n",
    "#plt.errorbar(x, y, yerr = error, ls='-.', fmt='o', capsize=3)\n",
    "plt.xlabel('Max_Features', fontsize=15)\n",
    "plt.ylabel('Accuracy', fontsize=15)\n",
    "plt.title('Change in Accuracy as max_features increases')\n",
    "plt.ylim(0.8, 0.86)\n",
    "plt.xlim(0,100)#plot the outputs\n",
    "\n",
    "z1 = np.array(y_above)\n",
    "z2 = np.array(y_below)\n",
    "\n",
    "plt.fill_between(x,y_above,y_below,where=z1>=z2,color='grey',alpha=0.5, interpolate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-survivor",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf =KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "score = cross_val_score(tree.DecisionTreeClassifier(criterion='gini', splitter = 'best', max_depth=9, min_samples_split = 0.6, ccp_alpha = 0.012, max_features = 100), x, y, cv= kf, scoring=\"accuracy\")\n",
    "print(f'Scores for each fold are: {score}')\n",
    "print(f'Average score: {\"{:.2f}\".format(score.mean())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores ={}\n",
    "x = list()\n",
    "y = list()\n",
    "error = list()\n",
    "\n",
    "ccp_alpha_vals = np.arange(0,0.3,0.001)\n",
    "\n",
    "\n",
    "for i in ccp_alpha_vals: #first create a for loop for each max depth\n",
    "    value_of_j = [] #this will take the average score for each value of k\n",
    "\n",
    "    for j in range(2,10): #create a for loop for each kfold\n",
    "        kf =KFold(n_splits=j, shuffle=True, random_state=42)\n",
    "        score = cross_val_score(tree.DecisionTreeClassifier(criterion='gini',splitter = 'best', max_depth=3, min_samples_split = 0.1, max_features=100, ccp_alpha=i), x_train, y_train, cv= kf, scoring=\"accuracy\")\n",
    "        value_of_j.append(score.mean())\n",
    "\n",
    "    res = {str(i): value_of_j}\n",
    "    scores.update(res) # for each max_depth, the average k_fold score\n",
    "\n",
    "get_min_sample_split = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "x =[]\n",
    "y =[]\n",
    "error = []\n",
    "for key, value in get_min_sample_split.items():\n",
    "    x.append(key)\n",
    "    y.append(sum(value)/len(value))\n",
    "    error.append(statistics.stdev(value))\n",
    "    \n",
    "x = [round(float(i),2) for i in x]\n",
    "    \n",
    "# plt.plot(x, error, 'or')\n",
    "plt.plot(x, y, color=(0.2, 0.4, 0.6, 0.6), lw=3)\n",
    "#plt.errorbar(x, y, yerr = error, barsabove = False, fmt='o', capsize=3, color='red')\n",
    "#plt.errorbar(x, y, yerr = error, ls='-.', fmt='o', capsize=3)\n",
    "plt.xlabel('ccp_alpha', fontsize=15)\n",
    "plt.ylabel('Accuracy', fontsize=15)\n",
    "plt.title('Change in Accuracy as ccp_alpha increases')\n",
    "plt.ylim(0.8, 0.86)\n",
    "plt.xlim(0,0.15)#plot the outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-encounter",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Build the Decision tree #2** - applying pre-pruning techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-detection",
   "metadata": {},
   "source": [
    "**K-Fold Classification** \n",
    "This will increase the number of times the test and train data is split, then will average out the accuracy scores. The cells below will increase the number of K up to 12, then plot the accuracy for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-football",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kfold_scores = {}\n",
    "for i in range(2, 12):\n",
    "    kf =KFold(n_splits=i, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(tree.DecisionTreeClassifier(criterion='gini', splitter = 'best', max_depth=3, min_samples_split = 0.1, max_features=100, ccp_alpha=0.01), x_test, y_test, cv= kf, scoring=\"accuracy\")\n",
    "    res = {str(i):list(score)}\n",
    "    kfold_scores.update(res)\n",
    "\n",
    "x =[]\n",
    "y =[]\n",
    "error = []\n",
    "for key, value in kfold_scores.items():\n",
    "    x.append(key)\n",
    "    y.append(sum(value)/len(value))\n",
    "    error.append(statistics.stdev(value))\n",
    "    \n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "# plt.plot(x, error, 'or')\n",
    "plt.bar(x, y, color=(0.2, 0.4, 0.6, 0.6))\n",
    "plt.errorbar(x, y, yerr = error, barsabove = False, fmt='o', capsize=3, color='red')\n",
    "#plt.errorbar(x, y, yerr = error, ls='-.', fmt='o', capsize=3)\n",
    "plt.xlabel('K', fontsize=15)\n",
    "plt.ylabel('Accuracy', fontsize=15)\n",
    "plt.title('Change in Accuracy as K increases')\n",
    "plt.ylim(0.7, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-niger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kf =KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "score = cross_val_score(tree.DecisionTreeClassifier(criterion='gini', max_depth=3, min_samples_split = 0.1, max_features=100, ccp_alpha=0.01, splitter = 'best'), x_test, y_test, cv= kf, scoring=\"accuracy\")\n",
    "print(f'Scores for each fold are: {score}')\n",
    "print(f'Average score: {\"{:.2f}\".format(score.mean())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-bandwidth",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = tree.DecisionTreeClassifier(criterion='gini', max_depth=4, min_samples_split = 0.1, max_features=100, ccp_alpha=0.01, splitter = 'best')\n",
    "dt.fit(x_train, y_train)\n",
    "y_train_predict = dt.predict(x_train)\n",
    "y_test_predict = dt.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-position",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "def plot_confusionmatrix(train_p, train, dom):\n",
    "    cf = confusion_matrix(train_p,train)\n",
    "    sns.heatmap(cf,annot=True,yticklabels=classes,xticklabels=classes,cmap='Blues', fmt='g')\n",
    "    plt.title(f'{dom} Confusion matrix')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f'{dom} Confusion matrix.png')\n",
    "    plt.show()\n",
    "    \n",
    "plot_confusionmatrix(y_train_predict,y_train,dom='Train')\n",
    "plot_confusionmatrix(y_test_predict,y_test,dom='Test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}