{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "literary-massage",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "**Communities and Crime Data set**\n",
    "\n",
    "Aim: Predicting the number of violent crimes per 100k population.\n",
    "\n",
    "Data from: https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-vaccine",
   "metadata": {},
   "source": [
    "1. Split test/ training data and build decision tree.\n",
    "2. Feature Selection. Discard the least important variables to reduce noise. Good variables are often constructed using ratios, differences, averages of variables etc.\n",
    "3. Pre-pruning\n",
    "4. Post-pruning (cost complexity pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "theoretical-official",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import sklearn.feature_selection\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV # RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accessory-pulse",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-371d9819729a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.70\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# check the dataset to see how much the dataset has grown (if dimenstionality was increased)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test data sets using sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.70, random_state=1)\n",
    "\n",
    "# check the dataset to see how much the dataset has grown (if dimenstionality was increased)\n",
    "print(data.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-stereo",
   "metadata": {},
   "source": [
    "**Feature Selection**\n",
    "- Build the model using the pre-processed data\n",
    "\n",
    "In order to avoid overfitting and slow computing (due to the increase in features from dummying as well as increasing dimensionality), selecting the most important features is important. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-orbit",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select k best is a univariate method for feature selection:\n",
    "# looks at the outcome and the relationship with each feature and selects k number of best features \n",
    "select = sklearn.feature_selection.SelectKBest(k=20)\n",
    "selected_features = select.fit(x_train, y_train)\n",
    "indices_selected = selected_features.get_support(indices=True)\n",
    "colnames_selected = [x.columns[I] for I in indices_selected]\n",
    "\n",
    "x_train_selected = x_train[colnames_selected]\n",
    "x_test_selected = x_test[colnames_selected]\n",
    "\n",
    "colnames_selected # 20 features selected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-woman",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "scrolled": false
   },
   "source": [
    "**Build Decision tree #1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-buffalo",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot tree at depth 2\n",
    "dtree = DecisionTreeClassifier(criterion='gini', max_depth=2)  # use the entropy rather than gini index\n",
    "dtree = dtree.fit(x_train, y_train)\n",
    "tree.plot_tree(dtree, feature_names=data.columns) \n",
    "\n",
    "# model can then be used to predict !\n",
    "# crime_predict = dtree.predict([[0,1]])\n",
    "\n",
    "# Assess accuracy at this stage\n",
    "clf = DecisionTreeClassifier()  # Create Decision Tree classifer object\n",
    "clf = clf.fit(x_train,y_train)  # Train Decision Tree Classifer\n",
    "y_pred = clf.predict(x_test)  # Predict the response for test dataset\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-handling",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# most stylised tree:\n",
    "# import graphviz\n",
    "# dot_data = tree.export_graphviz(dtree, out_file=None, feature_names=features, filled=True, rounded=True, special_characters=True)\n",
    "# graph = graphviz.Source(dot_data)\n",
    "# graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-encounter",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Build the Decision tree #2** - applying pre-pruning techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-fraction",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Testing pre pruning\n",
    "\n",
    "params = {'max_depth': [2,4,6,8,10,12],  # why did we select these parameters?\n",
    "         'min_samples_split': [2,3,4],\n",
    "         'min_samples_leaf': [1,2]}\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "gcv = GridSearchCV(estimator=clf,param_grid=params)\n",
    "gcv = gcv.fit(x_train,y_train)\n",
    "y_pred = gcv.predict(x_test)  # Predict the response for test dataset\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)) # better accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-pruning summary\n",
    "\n",
    "classes = ['Low Violence', 'High Violence']\n",
    "def plot_confusionmatrix(train_pred, train, dom):\n",
    "    print(f'{dom} Confusion matrix')\n",
    "    cf = confusion_matrix(train_pred,train)\n",
    "    sns.heatmap(cf,annot=True,yticklabels=classes\n",
    "               ,xticklabels=classes,cmap='Blues', fmt='g')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "model = gcv.best_estimator_\n",
    "model.fit(x_train,y_train)\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "print(f'Train score {accuracy_score(y_train_pred,y_train)}')\n",
    "print(f'Test score {accuracy_score(y_test_pred,y_test)}')\n",
    "plot_confusionmatrix(y_train_pred,y_train,dom='Train')\n",
    "plot_confusionmatrix(y_test_pred,y_test,dom='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-quality",
   "metadata": {},
   "source": [
    "**Build Decision Tree #3** - applying post-pruning techniques\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py\n",
    "\n",
    "Total impurity of leaves vs effective alphas of pruned tree:\n",
    "\n",
    "Minimal cost complexity pruning recursively finds the node with the “weakest link”. The weakest link is characterized by an effective alpha, where the nodes with the smallest effective alpha are pruned first.\n",
    "\n",
    "Scikit-learn provides DecisionTreeClassifier.cost_complexity_pruning_path that returns the effective alphas and the corresponding total leaf impurities at each step of the pruning process. As alpha increases, more of the tree is pruned, which increases the total impurity of its leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cost complexity pruning path\n",
    "path = clf.cost_complexity_pruning_path(x_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\")\n",
    "# the maximum effective alpha value is removed, because it is the trivial tree with only one node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train a decision tree using the effective alphas.\n",
    "# The last value in ccp_alphas is the alpha value that prunes the whole tree, \n",
    "# this leaves the tree (clfs[-1]) with one node.\n",
    "\n",
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(x_train, y_train)\n",
    "    clfs.append(clf)\n",
    "print(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "      clfs[-1].tree_.node_count, ccp_alphas[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Remove the last element in clfs and ccp_alphas, because it is the trivial tree with only one node.\n",
    "clfs = clfs[:-1]\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "# Show that the number of nodes and tree depth decreases as alpha increases.\n",
    "node_counts = [clf.tree_.node_count for clf in clfs]\n",
    "depth = [clf.tree_.max_depth for clf in clfs]\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "ax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\n",
    "ax[0].set_xlabel(\"alpha\")\n",
    "ax[0].set_ylabel(\"number of nodes\")\n",
    "ax[0].set_title(\"Number of nodes vs alpha\")\n",
    "ax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\n",
    "ax[1].set_xlabel(\"alpha\")\n",
    "ax[1].set_ylabel(\"depth of tree\")\n",
    "ax[1].set_title(\"Depth vs alpha\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compare accuracy vs alpha for training\n",
    "# When ccp_alpha is set to zero, the tree overfits; leading to a 100% training accuracy and 88% testing accuracy.\n",
    "# As alpha increases, more of the tree is pruned, thus creating a decision tree that generalizes better. \n",
    "\n",
    "train_scores = [clf.score(x_train, y_train) for clf in clfs]\n",
    "test_scores = [clf.score(x_test, y_test) for clf in clfs]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-permission",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit the model again with the most suitable alpha i.e. 0.008\n",
    "# this alpha achieves the best test accuracy\n",
    "\n",
    "tree = DecisionTreeClassifier(ccp_alpha=0.088, random_state=40)\n",
    "tree.fit(x_train, y_train)\n",
    "y_train_pred=tree.predict(x_train)\n",
    "y_test_pred=tree.predict(x_test)\n",
    "\n",
    "print(\"Pruned tree train accuracy score: \", accuracy_score(y_train, y_train_pred),\n",
    "\"Pruned tree test accuracy score: \", accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra graphs needed \n",
    "# https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
